# 学习日志
# 日期：2024-11-23
# 实验名称：学习基于 pytorch 的线性回归模型

## 1. 实验目标
- 学习线性回归模型，对于一个问题我们往往会首先使用线性回归模型。

## 2. 线性回归模型的假设

- 首先，假设自变量和因变量之间的关系是线性的，
即因变量可以表示为自变量中元素的加权和，这里通常允许包含观测值的一些噪声；
- 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。

## 3. 线性回归的基本元素及其表示

数据
- 训练集：每行数据是一个样本，列数据是特征或者标签。

模型
- 权重决定了每个特征对我们预测值的影响。
- 偏置是指当所有特征都取值为0时，预测值应该为多少。
- 给定一个数据集，我们的目标是寻找模型的权重和偏置。

目标
- 通过训练集学习模型的参数，估计权重和偏重。

模型的度量
- 损失函数（loss function）能够量化目标的实际值与预测值之间的差距。 
- 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。
- 回归问题中最常用的损失函数是平方误差函数。

优化
- 优化算法能够帮助我们找到最小化损失函数的参数值。

我们采用随机梯度下降的优化算法，这是一个迭代的过程：
原理
- 通过不断地在损失函数递减的方向上更新参数来降低误差。

问题
- 但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。 
改进
- 我们通常会在每次需要计算更新的时候随机抽取一小批样本， 这种变体叫做小批量随机梯度下降（minibatch stochastic gradient descent）。

算法实现
- 初始化模型参数的值，如随机初始化；
- 从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。

机器学习的难点
- 深度学习实践者很少会去花费大力气寻找这样一组参数，使得在训练集上的损失达到最小。
- 更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为泛化（generalization）。

问题
- 计算很慢
改进
- 对计算进行矢量化，从而利用线性代数库

## 5. 从零开始实现线性回归

从零开始实现可以确保我们真正知道自己在做什么。

- 读取数据集
读取数据的最低要求是能让程序跑起来，所以它在格式上必须匹配我们的神经网络模型。
我们的数据集要分成两部分：特征和标签；而它要返回的是一个小批量迭代器。
特征的形状是 (样本数, 特征数)，标签的形状是 (样本数, 标签量)。
我们的迭代器输入的是特征和标签张量构成的元组 (特征，标签)，小批量的大小，是否随机抽样；输出的是特征标签构成的列表 [特征，标签]。

读取数据的可选要求是可视化，这样我们可以更直观地了解数据的特点。
可以通过散点图来展示数据的分布情况。
还可以检验迭代器是否正常工作。

- 定义模型
只需要关心，输入的特征有多少，输出的标签有多少，模型有多少层（线性情况下只需要一层）。
- 初始化模型参数
- 定义损失函数
- 定义优化函数（小样本随机梯度下降）
- 训练

## 6. 整合（位置：scripts → modules）

- 线性回归的简洁实现
- 多输入多输出的线性网络是否有效？


## 7. 总结

学习如何实现单变量和多变量的线性网络搭建。
