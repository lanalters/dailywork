# 学习日志
# 日期：2024-11-24
# 实验名称：学习基于 pytorch 的 softmax 分类模型

## 1. 实验目标
- 学习 softmax 分类模型。

## 2. 分类问题的背景

我们从一个图像分类问题开始。 
如何选择特征：
- 我们可以用一个标量表示每个像素值，于是有像素级别的特征。
如何表示标签：
- 通常我们会用离散值来表示类别，如 {0, 1, 2, 3, 4}。
- 如果类别间有一些自然顺序，那么问题转变为回归问题。

网络架构
- 我们需要一个有多个输出的模型，每个输出对应一个类别。
- 最简单的想法是使用一个线性模型，即一个全连接层。

问题：全连接层的参数开销很大
- 假设输入特征的维度是 d，输出特征的维度是 q，那么全连接层的参数开销是 O(dq)。
改进：减少成本

## 3. softmax 回归的基本概念

目标
- 优化参数以最大化观测数据的概率。 
- 更具体地说，为了得到预测结果，我们将设置一个阈值，如选择具有最大概率的标签。

问题
- 我们不能将未规范化的预测直接视作我们感兴趣的输出。
- 因为，未规范化的预测值可能是负数，也可能是大于1的数。
解决
- softmax 函数能够将未规范化的预测变换为非负数并且总和为 1，同时让模型保持可导的性质。

## 4. 从零开始实现 softmax 回归

- 读取数据集
我们必须了解数据集的格式，以便我们可以将其转换为合适的格式。
将图片数据集转化为张量后，是 torch.utils.data.Dataset 的实例。
长度表示数据集的大小，即样本量。
索引表示样本的位置，返回值由特征和标签构成的元组。
特征是 1x28x28 的图像，标签是 0-9 的数字。

很容易构造小批量迭代器。
输入 torch.utils.data.DataLoader 类的实例和小样本大小，返回一个迭代器，小样本量的特征和标量组成的列表。

接下来是可选的可视化部分。
迭代器是否正常工作。
定义了一个 show_images 函数，它可以在绘制多个图像。
需要输入图像列表(batch_size × 像素)，图像展示的行和列，图像尺寸放缩比例；输出图像列表。

- 定义模型
有两层，第一层是展平层，第二层是全连接层。
- 初始化模型参数
- 定义损失函数
- 定义优化函数
- 训练
累计损失值、累计准确数、样本数。


## 5. 总结

softmax 回归是一个简单的神经网络，用于多分类问题。
