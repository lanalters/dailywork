# 学习日志
# 日期：2024-11-24
# 实验名称：学习基于 pytorch 的 mlp 模型

## 1. 实验目标

- 学习多层感知机模型。

## 2. mlp 模型的基本概念

线性模型的局限性
- 线性意味着单调性，即输出随着输入的增加而增加或减少。
- 关系复杂的数据集，线性模型无法很好地拟合。比如图像。
克服
- 加入非线性变换的层，如 ReLU。

简单介绍各种激活函数
ReLU 函数
- 缓解梯度消失问题。
sigmoid 函数
- 挤压函数
- 平滑、可微的阈值函数的近似
tanh 函数
- 平滑、可微的阈值函数的近似
- 关于原点对称

## 3. 从零开始实现 mlp 模型

- 读取数据集
- 初始化模型参数
- 定义模型（定义激活函数）
- 定义损失函数
- 定义优化函数
- 训练

## 4. mlp 模型的简洁实现

- 读取数据集
- 定义模型
- 初始化模型参数
- 定义损失函数
- 定义优化函数
- 训练

## 5. mlp 模型应用中的问题

过拟合
- 模型在训练集上表现很好，但在测试集上表现很差，说明模型过拟合。
克服
- 正则化

一般假设样本是独立同分布的
- 但在现实中，样本可能不满足这个假设。

产生过拟合的可能的原因
- 可调整参数的数量太多
- 训练样本数量太少
- 权重的取值范围太大

如何选择模型
在我们希望比较具有不同数量的隐藏层，不同数量的隐藏单元和不同的学习率的模型时，我们如何选择模型？
- 验证集

## 6. 正则化技术——权重衰减

原理
- 通过控制函数与零的差距来限制模型的复杂度。
- 训练目标改为最小化损失函数和惩罚项的和。

选择惩罚项
- L2 范数：ridge 岭回归算法
- L1 范数：Lasso 套索回归算法
实践中
- L2 算法使得我们的学习算法偏向于在大量特征上均匀分布权重的模型
- L1 算法倾向于导致模型将权重集中在少数特征上。


## 7. 正则化技术——暂退法

泛化性和灵活性之间的基本权衡被称为偏差-方差权衡。
- 线性函数有很大的偏差，但是有很小的方差。
- 即使我们有比特征多得多的样本，深度神经网络也可能过拟合。

经典泛化理论认为，为例缩小训练性能和测试性能之间的差距，我们需要控制模型的复杂度。
而简单性的另一个角度就是平滑。
- 训练时在计算后续层之前向网络的每一层注入噪声，在输入-输出映射上增强平滑性。
- 标准的暂退法包括在计算下一层之前将当前层的一些节点置零。

如何注入噪声
- 一种想法是无偏
- 具体而言，按照保留的节点的分数进行规范化来消除每一层的偏差。

测试时通常不使用暂退法，也有例外
- 估计神经网络的不确定性




